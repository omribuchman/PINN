{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a766fe-5412-4c5b-8de1-850d922e1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://georgemilosh.github.io/blog/2022/distill/\n",
    "# check: https://stackoverflow.com/questions/64995683/loss-function-with-derivative-in-tensorflow-2\n",
    "# NOTE - x_train[:i] - all samples in TS up to i , not the i'th sample !!\n",
    "# u(x,t)\n",
    "# Burgers eq.: u_t+uu_x-(0.001)u_xx = 0\n",
    "# BC         : u(x,0) = sin(x)\n",
    "# IC         : u(-1,t) = i(1,t) = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626835c5-e9aa-4710-ab20-df2a2ee7997a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f266a5c-3e0a-4a6b-a170-8c2d16fe9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundery(x):\n",
    "    return [np.sin(x)]      \n",
    "\n",
    "def construct_training_set_BC(dx):\n",
    "    x = np.pi*np.arange(-1, 1.1 , 0.1, dtype = np.float32)\n",
    "    zeros = np.zeros(np.shape(x))\n",
    "    y_train_BC = np.array(boundery(x))[0]\n",
    "    x_train_BC = np.vstack((x, zeros)).T\n",
    "    #print ('x_train_BC type = ',type(x_train_BC))\n",
    "    #print ('y_train_BC type = ',type(y_train_BC))\n",
    "    #print ('x_train_BC = ',x_train_BC)\n",
    "    #print ('y_train_BC = ',y_train_BC)\n",
    "    return x_train_BC,y_train_BC\n",
    "\n",
    "def construct_training_set_IC(dt):\n",
    "    t =  (np.arange(0, 1.0001 , dt, dtype = np.float32))\n",
    "    Nt = np.shape(t)\n",
    "    x1 = np.ones(Nt)\n",
    "    xminus1 = x1-2.0\n",
    "    x1_t = np.vstack((x1, t)).T.tolist()\n",
    "    x_minus1_t = np.vstack((xminus1, t)).T.tolist()\n",
    "    x_train_IC = np.array( x_minus1_t+x1_t)\n",
    "    y_train_IC = np.zeros(np.shape(x_train_IC)[0])\n",
    "    #print ('x_train_IC = ',x_train_IC)\n",
    "    #print ('y_train_IC = ',y_train_IC)\n",
    "    #print (np.shape(x_train_IC)[0])\n",
    "    return x_train_IC,y_train_IC\n",
    "            \n",
    "def construct_training_set(dx,dt):\n",
    "    x_train_BC,y_train_BC = construct_training_set_BC(dx)\n",
    "    x_train_IC,y_train_IC = construct_training_set_IC(dt)    \n",
    "    x_train =  np.array(x_train_BC.tolist() + x_train_IC.tolist())\n",
    "    y_train =  np.array(y_train_BC.tolist() + y_train_IC.tolist())\n",
    "    #print ('x_train = ',x_train)    \n",
    "    #print ('y_train = ',y_train)\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31e6ecf-4cc3-414c-af07-628112730701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(num_hidden_layers=1, num_neurons_per_layer=2):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(2,))) \n",
    "    # Append hidden layers\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "            activation=tf.keras.activations.exponential,\n",
    "            kernel_initializer='glorot_normal'))\n",
    "    # Output is one-dimensional\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    #print (model.get_layer(index=1))\n",
    "    return model\n",
    "\n",
    "def set_weights_for_specific_model_layer(model):\n",
    "    layer_weights_shape = [w.shape for w in model.get_weights()]\n",
    "    #print(f\"layers weight shapes: {layer_weights_shape}\")\n",
    "    custom_kernel = np.ones(layer_weights_shape[0]) \n",
    "    custom_bias = np.zeros(layer_weights_shape[1])\n",
    "    model.set_weights([custom_kernel, custom_bias]) \n",
    "    \n",
    "def set_weights_as_one_for_model(model):\n",
    "    current_weights = model.get_weights()\n",
    "    new_weights = [np.ones_like(w) for w in current_weights]\n",
    "    model.set_weights(new_weights)  \n",
    "\n",
    "def set_internal_variables_for_model(model,m,b):\n",
    "    tm = tf.convert_to_tensor(m)\n",
    "    tb = tf.convert_to_tensor(b)\n",
    "    first_layer  = model.layers[0]\n",
    "    secend_layer = model.layers[1]\n",
    "    set_weights_for_specific_model_layer(first_layer)\n",
    "    set_weights_for_specific_model_layer(secend_layer)\n",
    "    dense_layer = model.get_layer(index=0)\n",
    "    dense_layer.set_weights([tm,tb])\n",
    "    #print (' network trainable_variables  = ')\n",
    "    #print( model.trainable_variables)\n",
    "    #print ('')\n",
    "    #print (' W1  = ')\n",
    "    #print( model.trainable_variables[0].numpy())\n",
    "    #print (' b1 = ')\n",
    "    #print( model.trainable_variables[1].numpy())\n",
    "    #print (' W2 = ')\n",
    "    #print( model.trainable_variables[2].numpy())\n",
    "    #print (' b2 = ')\n",
    "    #print( model.trainable_variables[3].numpy())\n",
    "\n",
    "def excat_solution(model,r):\n",
    "    w11=model.trainable_variables[0][0][0].numpy()\n",
    "    w12=model.trainable_variables[0][0][1].numpy()\n",
    "    w21=model.trainable_variables[0][1][0].numpy()\n",
    "    w22=model.trainable_variables[0][1][1].numpy()\n",
    "    w13=model.trainable_variables[2][0].numpy()\n",
    "    w23=model.trainable_variables[2][1].numpy()\n",
    "    x = r.numpy()[0][0]\n",
    "    y = r.numpy()[0][1]\n",
    "    # print ('w11 w12 = ',w11,w12)\n",
    "    # print ('w21,w22 = ',w21,w22)    \n",
    "    # print ('w13,w23 = ',w13,w23)\n",
    "    # print ('x, y = ',x,y)\n",
    "    du_dx_dx = w13*w11**2*np.exp(w11*x + w21*y) + w23*w12**2*np.exp(w12*x + w22*y) \n",
    "    du_dy_dy = w13*w21**2*np.exp(w11*x + w21*y) + w23*w22**2*np.exp(w12*x + w22*y)    \n",
    "    print ('exact du_dx_dx, du_dy_dy = ',du_dx_dx,du_dy_dy)\n",
    "    return (du_dx_dx,du_dy_dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52617508-c6cf-4b74-8aa8-1de5b11ce3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size',name='mean_squared_error')\n",
    "\n",
    "def custom_mse_loss(y_true, y_pred):\n",
    "    l_BC_IC = loss_fn(y_true,y_pred)\n",
    "    #squared_difference = tf.square(y_true - y_pred) \n",
    "    #return tf.reduce_mean(squared_difference, axis=-1)\n",
    "    return l_BC_IC\n",
    "\n",
    "class Custom_PINN_Loss(Loss):\n",
    "    def __init__(self,model,x , name='Custom_PINN_Loss',**kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.x =x\n",
    "        self.model=model\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        l1 = loss_fn(y_true, y_pred)\n",
    "        # x = tf.constant([[1,2]], dtype=tf.float32)\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch(self.x)  # \"The input x must be a tf.Variable or explicitly \"watched\" using tape.watch(x) to be tracked by GradientTape \"    \n",
    "            with tf.GradientTape(persistent=True) as tape1:\n",
    "                tape1.watch(self.x)\n",
    "                u = model(self.x)\n",
    "                print('u = ',u)\n",
    "                grad_u = tape1.gradient(u, self.x)        \n",
    "            hessian_u = tape2.jacobian(grad_u, self.x)    \n",
    "        ux = grad_u[0][0]\n",
    "        ut = grad_u[0][1]\n",
    "        uxx = hessian_u[0][0][0][0]\n",
    "        #print ('gradient: ',grad_u)\n",
    "        #print ('hessian',hessian_u)\n",
    "        #print ('ux',ux)\n",
    "        #print ('ut',ut)\n",
    "        #print ('u',u)  \n",
    "        l2 = ut + u*ux - (0.001)*uxx\n",
    "        #print ('loss2 = ',loss2)       \n",
    "        return l1+l2\n",
    "       \n",
    "    # Optional: Implement get_config to save and load the model with custom parameters\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config['threshold'] = self.threshold\n",
    "        return base_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5874aa8f-6f59-4d5c-9593-12f43762051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#                                                     M    A     I     N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6002b0a-7ed5-4aa5-83cf-ad8b592eb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = construct_training_set(0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ccfc1f-2482-42f6-9e1b-c4f4807eefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "#model.summary()\n",
    "m = [[1,2],[3,4]]\n",
    "b = [0,0]\n",
    "set_internal_variables_for_model(model,m,b)\n",
    "x1 = tf.constant([[1,2]], dtype=tf.float32)\n",
    "#excat_solution(model,x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f66e53f-6ff6-41e1-b793-b1cffe08edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 4\n",
    "predictions = model(x_train[:sample_number]).numpy()\n",
    "#print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "609ff9c8-d009-48c4-899b-bb6a4ceb4f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u =  tf.Tensor([[21998.096]], shape=(1, 1), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000019F440132E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[9.401472e+08]]\n"
     ]
    }
   ],
   "source": [
    "#print ('custom_mse_loss = ',custom_mse_loss(y_train[:sample_number],predictions).numpy())\n",
    "#print ('custom_mse_loss = ',Custom_PINN_Loss(y_train[:sample_number],predictions,threshold=1.3,x=x1).numpy())\n",
    "CML = Custom_PINN_Loss(model,x1,model)\n",
    "print (CML.call(y_train[:sample_number],predictions).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae76c676-0a68-45cb-a522-a46fca4a6157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "u =  Tensor(\"compile_loss/Custom_PINN_Loss/sequential_1/dense_1_2/Add:0\", shape=(1, 1), dtype=float32)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "gradient:  Tensor(\"gradient_tape/compile_loss/Custom_PINN_Loss/sequential_1/dense_1/MatMul/MatMul:0\", shape=(1, 2), dtype=float32)\n",
      "hessian Tensor(\"compile_loss/Custom_PINN_Loss/Reshape_1:0\", shape=(1, 2, 1, 2), dtype=float32)\n",
      "u =  Tensor(\"compile_loss/Custom_PINN_Loss/sequential_1/dense_1_2/Add:0\", shape=(1, 1), dtype=float32)\n",
      "gradient:  Tensor(\"gradient_tape/compile_loss/Custom_PINN_Loss/sequential_1/dense_1/MatMul/MatMul:0\", shape=(1, 2), dtype=float32)\n",
      "hessian Tensor(\"compile_loss/Custom_PINN_Loss/Reshape_1:0\", shape=(1, 2, 1, 2), dtype=float32)\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 1042248192.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1020562880.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 999340288.0000 \n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 978584576.0000 \n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 958308096.0000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19f40162690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.compile(optimizer='adam',loss=custom_mse_loss,metrics=['accuracy']) #  loss=custom_mse_loss\n",
    "model.compile(optimizer='adam', loss=Custom_PINN_Loss(model,x1))\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ca64d-dbdc-4259-aa6a-86bdcb277471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
